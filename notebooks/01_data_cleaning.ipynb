{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39e3a7ec-8051-4d0c-a2f4-324ffc4ac5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and load relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2abbd96f-71ca-4eeb-b3c7-4383fead2421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the Cyclistic datasets (csv files) from the past 12 months\n",
    "jan_trips = pd.read_csv(\"2025_01_cyclistic.csv\")\n",
    "feb_trips = pd.read_csv(\"2025_02_cyclistic.csv\")\n",
    "mar_trips = pd.read_csv(\"2025_03_cyclistic.csv\")\n",
    "apr_trips = pd.read_csv(\"2025_04_cyclistic.csv\")\n",
    "may_trips = pd.read_csv(\"2025_05_cyclistic.csv\")\n",
    "june_trips = pd.read_csv(\"2025_06_cyclistic.csv\")\n",
    "july_trips = pd.read_csv(\"2025_07_cyclistic.csv\")\n",
    "aug_trips = pd.read_csv(\"2025_08_cyclistic.csv\")\n",
    "sep_trips = pd.read_csv(\"2025_09_cyclistic.csv\")\n",
    "oct_trips = pd.read_csv(\"2025_10_cyclistic.csv\")\n",
    "nov_trips = pd.read_csv(\"2025_11_cyclistic.csv\")\n",
    "dec_trips = pd.read_csv(\"2025_12_cyclistic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e059a6c7-23d8-43d8-a62a-e503e9bac017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking column consistency across all months...\n",
      "‚úÖ All 12 months have identical column structure\n",
      "Columns (15): ['ride_id', 'ride_length', 'start_station_id', 'start_lng', 'day_of_week', 'end_station_id', 'member_casual', 'end_lng', 'ended_at', 'end_station_name', 'start_station_name', 'end_lat', 'rideable_type', 'started_at', 'start_lat']\n"
     ]
    }
   ],
   "source": [
    "# Check if all DataFrames have identical columns\n",
    "print(\"Checking column consistency across all months...\")\n",
    "\n",
    "# Get column sets for each month\n",
    "datasets = {\n",
    "    'Jan': jan_trips, 'Feb': feb_trips, 'Mar': mar_trips,\n",
    "    'Apr': apr_trips, 'May': may_trips, 'Jun': june_trips,\n",
    "    'Jul': july_trips, 'Aug': aug_trips, 'Sep': sep_trips,\n",
    "    'Oct': oct_trips, 'Nov': nov_trips, 'Dec': dec_trips\n",
    "}\n",
    "\n",
    "# Check column names match\n",
    "base_columns = set(jan_trips.columns)\n",
    "all_match = True\n",
    "\n",
    "for month, df in datasets.items():\n",
    "    if set(df.columns) != base_columns:\n",
    "        print(f\"‚ö†Ô∏è  {month} has different columns!\")\n",
    "        print(f\"   Extra: {set(df.columns) - base_columns}\")\n",
    "        print(f\"   Missing: {base_columns - set(df.columns)}\")\n",
    "        all_match = False\n",
    "\n",
    "if all_match:\n",
    "    print(\"‚úÖ All 12 months have identical column structure\")\n",
    "    print(f\"Columns ({len(base_columns)}): {list(base_columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd8ab08c-b19f-4c82-8e58-1fecf44bf032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Combined all 12 months: 5,552,994 total rows\n",
      "Memory usage: 3441.8 MB\n"
     ]
    }
   ],
   "source": [
    "# Stack individual month's data frames into one big data frame\n",
    "all_trips = pd.concat([\n",
    "    jan_trips, feb_trips, mar_trips, apr_trips, may_trips, june_trips,\n",
    "    july_trips, aug_trips, sep_trips, oct_trips, nov_trips, dec_trips\n",
    "], ignore_index=True)\n",
    "\n",
    "print(f\"‚úÖ Combined all 12 months: {len(all_trips):,} total rows\")\n",
    "print(f\"Memory usage: {all_trips.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9b7ffca-8e3a-46be-b07e-9bb0dd73da39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined raw data to: 2025_cyclistic_combined_raw.csv\n"
     ]
    }
   ],
   "source": [
    "# Save combined raw data before cleaning\n",
    "raw_data_path = '2025_cyclistic_combined_raw.csv'\n",
    "all_trips.to_csv(raw_data_path, index=False)\n",
    "print(f\"Saved combined raw data to: {raw_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62681058-3080-4b71-8081-b28d52600932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Loading saved raw data...\n",
      "Loaded 5,552,994 rows, 15 columns\n",
      "Date range in raw data: 1/21/2025  5:23:55 PM to 12/20/2025   1:08:28 PM\n"
     ]
    }
   ],
   "source": [
    "# 1. Load raw data\n",
    "print(\"\\n1. Loading saved raw data...\")\n",
    "df = pd.read_csv('2025_cyclistic_combined_raw.csv')\n",
    "print(f\"Loaded {len(df):,} rows, {len(df.columns)} columns\")\n",
    "print(f\"Date range in raw data: {df['started_at'].iloc[0]} to {df['started_at'].iloc[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8361ae63-db7d-40cb-b95b-af827f057ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Converting datetime columns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_1824\\4028878929.py:3: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['started_at'] = pd.to_datetime(df['started_at'], errors='coerce')\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_1824\\4028878929.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['ended_at'] = pd.to_datetime(df['ended_at'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All datetime conversions successful\n"
     ]
    }
   ],
   "source": [
    "# 2. Convert datetime columns with error handling\n",
    "print(\"\\n2. Converting datetime columns...\")\n",
    "df['started_at'] = pd.to_datetime(df['started_at'], errors='coerce')\n",
    "df['ended_at'] = pd.to_datetime(df['ended_at'], errors='coerce')\n",
    "\n",
    "# Check for failed conversions\n",
    "failed_start = df['started_at'].isna().sum()\n",
    "failed_end = df['ended_at'].isna().sum()\n",
    "\n",
    "if failed_start > 0 or failed_end > 0:\n",
    "    print(f\"Warning: {failed_start} started_at and {failed_end} ended_at conversions failed\")\n",
    "    print(\"These rows will be removed later.\")\n",
    "else:\n",
    "    print(\"All datetime conversions successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bc8c8d2-eb4d-4982-8058-2afddfd1ed89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Standardizing member_casual column...\n",
      "Before standardization:\n",
      "member_casual\n",
      "member    3553497\n",
      "casual    1999497\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After standardization:\n",
      "member_casual\n",
      "member    3553497\n",
      "casual    1999497\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 3. Standardize member_casual column\n",
    "print(\"\\n3. Standardizing member_casual column...\")\n",
    "\n",
    "print(\"Before standardization:\")\n",
    "print(df['member_casual'].value_counts(dropna=False))\n",
    "\n",
    "# Standardize: strip whitespace, lowercase\n",
    "df['member_casual'] = df['member_casual'].astype(str).str.strip().str.lower()\n",
    "\n",
    "print(\"\\nAfter standardization:\")\n",
    "print(df['member_casual'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e785245-cdb2-44ac-a0f4-eb0ba4c432af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Converting ride_length to minutes...\n",
      "ride_length sample values:\n",
      "['0:13:57', '0:05:04', '0:11:35']\n",
      "\n",
      "Conversion check (first 5 rows):\n",
      "  ride_length  ride_length_min\n",
      "0     0:13:57        13.950000\n",
      "1     0:05:04         5.066667\n",
      "2     0:11:35        11.583333\n",
      "3     0:03:34         3.566667\n",
      "4     0:02:35         2.583333\n",
      "\n",
      "Ride length statistics (minutes):\n",
      "  Min: 0.00\n",
      "  Max: 1574.90\n",
      "  Mean: 16.03\n",
      "  Missing: 29\n"
     ]
    }
   ],
   "source": [
    "# 4. Convert ride_length to minutes\n",
    "print(\"\\n4. Converting ride_length to minutes...\")\n",
    "\n",
    "# Check current format of ride_length\n",
    "print(f\"ride_length sample values:\")\n",
    "print(df['ride_length'].head(3).tolist())\n",
    "\n",
    "# Function to convert hh:mm:ss to minutes\n",
    "def convert_ride_length(time_str):\n",
    "    \"\"\"Convert hh:mm:ss string to total minutes (float)\"\"\"\n",
    "    try:\n",
    "        if pd.isna(time_str):\n",
    "            return None\n",
    "        \n",
    "        # Handle different formats\n",
    "        if isinstance(time_str, str):\n",
    "            # Format: \"hh:mm:ss\" or \"mm:ss\"\n",
    "            parts = time_str.split(':')\n",
    "            \n",
    "            if len(parts) == 3:  # hh:mm:ss\n",
    "                hours, minutes, seconds = map(int, parts)\n",
    "                return hours * 60 + minutes + seconds / 60\n",
    "            elif len(parts) == 2:  # mm:ss\n",
    "                minutes, seconds = map(int, parts)\n",
    "                return minutes + seconds / 60\n",
    "            else:\n",
    "                return None\n",
    "        elif isinstance(time_str, (int, float)):\n",
    "            # If already numeric, assume it's seconds\n",
    "            return time_str / 60\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Apply conversion\n",
    "df['ride_length_min'] = df['ride_length'].apply(convert_ride_length)\n",
    "\n",
    "print(f\"\\nConversion check (first 5 rows):\")\n",
    "print(df[['ride_length', 'ride_length_min']].head())\n",
    "\n",
    "print(f\"\\nRide length statistics (minutes):\")\n",
    "print(f\"  Min: {df['ride_length_min'].min():.2f}\")\n",
    "print(f\"  Max: {df['ride_length_min'].max():.2f}\")\n",
    "print(f\"  Mean: {df['ride_length_min'].mean():.2f}\")\n",
    "print(f\"  Missing: {df['ride_length_min'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e291756-c763-417f-8d19-8dfd3e186f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Converting day_of_week numbers to day names...\n",
      "Current day_of_week values (1-7, Sunday=1):\n",
      "day_of_week\n",
      "1    714105\n",
      "2    731020\n",
      "3    788657\n",
      "4    771691\n",
      "5    834050\n",
      "6    849067\n",
      "7    864404\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Day distribution (from numeric column):\n",
      "day_name\n",
      "Friday       849067\n",
      "Monday       731020\n",
      "Saturday     864404\n",
      "Sunday       714105\n",
      "Thursday     834050\n",
      "Tuesday      788657\n",
      "Wednesday    771691\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 5. Convert day_of_week to day names\n",
    "print(\"\\n5. Converting day_of_week numbers to day names...\")\n",
    "\n",
    "# Check current day_of_week values\n",
    "print(f\"Current day_of_week values (1-7, Sunday=1):\")\n",
    "print(df['day_of_week'].value_counts().sort_index())\n",
    "\n",
    "# Map numeric to day names\n",
    "day_map = {\n",
    "    1: 'Sunday',\n",
    "    2: 'Monday',\n",
    "    3: 'Tuesday',\n",
    "    4: 'Wednesday',\n",
    "    5: 'Thursday',\n",
    "    6: 'Friday',\n",
    "    7: 'Saturday'\n",
    "}\n",
    "\n",
    "# Create day_name column\n",
    "df['day_name'] = df['day_of_week'].map(day_map)\n",
    "\n",
    "print(f\"\\nDay distribution (from numeric column):\")\n",
    "print(df['day_name'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "906a1256-0ccf-4a83-9a0a-141c8d257749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. Adding additional time features...\n",
      "Added: month, year, hour, hour_category, date, season\n"
     ]
    }
   ],
   "source": [
    "# 6. Add additional time features\n",
    "print(\"\\n6. Adding additional time features...\")\n",
    "\n",
    "# Month and year\n",
    "df['month'] = df['started_at'].dt.month_name()\n",
    "df['year'] = df['started_at'].dt.year\n",
    "\n",
    "# Hour of day\n",
    "df['hour'] = df['started_at'].dt.hour\n",
    "df['hour_category'] = pd.cut(df['hour'], \n",
    "                             bins=[0, 6, 10, 15, 19, 24],\n",
    "                             labels=['Night (0-6)', 'Morning (7-10)', 'Midday (11-15)', \n",
    "                                     'Evening (16-19)', 'Night (20-23)'],\n",
    "                             right=False)\n",
    "\n",
    "# Date (without time)\n",
    "df['date'] = df['started_at'].dt.date\n",
    "\n",
    "# Season (based on month)\n",
    "def get_season(month_name):\n",
    "    \"\"\"Convert month_name to season with datetime - FIXED\"\"\"\n",
    "    # Convert to string first\n",
    "    month_str = str(month_name)\n",
    "    try:\n",
    "        month_num = datetime.strptime(month_str, \"%B\").month\n",
    "        if month_num in [12, 1, 2]:\n",
    "            return 'Winter'\n",
    "        elif month_num in [3, 4, 5]:\n",
    "            return 'Spring'\n",
    "        elif month_num in [6, 7, 8]:\n",
    "            return 'Summer'\n",
    "        else:\n",
    "            return 'Fall'\n",
    "    except:\n",
    "        return 'Unknown'\n",
    "\n",
    "df['season'] = df['month'].apply(get_season)\n",
    "\n",
    "print(\"Added: month, year, hour, hour_category, date, season\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cadbb432-dbf8-4eab-85c5-439f34698f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. Filtering invalid data...\n",
      "Original row count: 5,552,994\n",
      "  Condition 1: Removed 29 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_1824\\2010358690.py:23: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  clean_df = clean_df[condition].copy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Condition 2: Removed 146,892 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_1824\\2010358690.py:23: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  clean_df = clean_df[condition].copy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Condition 3: Removed 5,585 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_1824\\2010358690.py:23: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  clean_df = clean_df[condition].copy()\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_1824\\2010358690.py:23: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  clean_df = clean_df[condition].copy()\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_1824\\2010358690.py:23: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  clean_df = clean_df[condition].copy()\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_1824\\2010358690.py:23: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  clean_df = clean_df[condition].copy()\n",
      "C:\\Users\\Owner\\AppData\\Local\\Temp\\ipykernel_1824\\2010358690.py:23: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  clean_df = clean_df[condition].copy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FILTERING SUMMARY:\n",
      "  Original rows: 5,552,994\n",
      "  Removed rows: 152,506 (2.75%)\n",
      "  Final clean rows: 5,400,488\n"
     ]
    }
   ],
   "source": [
    "# 7. Filter invalid data\n",
    "print(\"\\n7. Filtering invalid data...\")\n",
    "\n",
    "original_count = len(df)\n",
    "print(f\"Original row count: {original_count:,}\")\n",
    "\n",
    "# Define filtering conditions\n",
    "conditions = [\n",
    "    df['ride_length_min'].notna(),                    # Has valid ride length\n",
    "    df['ride_length_min'] >= 1,                       # At least 1 minute\n",
    "    df['ride_length_min'] <= 1440,                    # Max 24 hours (1440 minutes)\n",
    "    df['member_casual'].isin(['member', 'casual']),   # Valid user type\n",
    "    df['started_at'].notna(),                         # Has start time\n",
    "    df['ended_at'].notna(),                           # Has end time\n",
    "    df['started_at'] < df['ended_at'],                # Logical time order\n",
    "    df['rideable_type'].notna()                       # Has bike type\n",
    "]\n",
    "\n",
    "# Apply all conditions\n",
    "clean_df = df.copy()\n",
    "for i, condition in enumerate(conditions, 1):\n",
    "    before = len(clean_df)\n",
    "    clean_df = clean_df[condition].copy()\n",
    "    removed = before - len(clean_df)\n",
    "    if removed > 0:\n",
    "        print(f\"  Condition {i}: Removed {removed:,} rows\")\n",
    "\n",
    "# Final count\n",
    "final_count = len(clean_df)\n",
    "removed_total = original_count - final_count\n",
    "removal_percent = (removed_total / original_count) * 100\n",
    "\n",
    "print(f\"\\nFILTERING SUMMARY:\")\n",
    "print(f\"  Original rows: {original_count:,}\")\n",
    "print(f\"  Removed rows: {removed_total:,} ({removal_percent:.2f}%)\")\n",
    "print(f\"  Final clean rows: {final_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b976ffae-b711-4f98-9772-de8324b43c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8. Generating data quality report...\n",
      "\n",
      "DATA QUALITY CHECKLIST:\n",
      "----------------------------------------\n",
      "1. User Type Distribution:\n",
      "   Member: 3,484,492 rides (64.5%)\n",
      "   Casual: 1,915,996 rides (35.5%)\n",
      "\n",
      "2. Date Range:\n",
      "   Start: 2024-12-31 18:54:42\n",
      "   End: 2025-12-31 23:53:24\n",
      "   Total days: 365\n",
      "\n",
      "3. Ride Length Statistics (minutes):\n",
      "   Member:\n",
      "     Avg: 12.2 min\n",
      "     Median: 8.7 min\n",
      "     Max: 1439.9 min\n",
      "   Casual:\n",
      "     Avg: 19.9 min\n",
      "     Median: 11.9 min\n",
      "     Max: 1440.0 min\n",
      "\n",
      "4. Missing Values in Clean Data:\n",
      "   start_station_name: 1095590 missing (20.29%)\n",
      "   start_station_id: 1095590 missing (20.29%)\n",
      "   end_station_name: 1126606 missing (20.86%)\n",
      "   end_station_id: 1126606 missing (20.86%)\n",
      "   end_lat: 159 missing (0.00%)\n",
      "   end_lng: 159 missing (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# 8. Data Quality Report\n",
    "print(\"\\n8. Generating data quality report...\")\n",
    "\n",
    "print(\"\\nDATA QUALITY CHECKLIST:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 1. User type distribution\n",
    "print(\"1. User Type Distribution:\")\n",
    "user_counts = clean_df['member_casual'].value_counts()\n",
    "for user_type, count in user_counts.items():\n",
    "    percent = (count / len(clean_df)) * 100\n",
    "    print(f\"   {user_type.title()}: {count:,} rides ({percent:.1f}%)\")\n",
    "\n",
    "# 2. Date range\n",
    "print(f\"\\n2. Date Range:\")\n",
    "print(f\"   Start: {clean_df['started_at'].min()}\")\n",
    "print(f\"   End: {clean_df['started_at'].max()}\")\n",
    "print(f\"   Total days: {(clean_df['started_at'].max() - clean_df['started_at'].min()).days}\")\n",
    "\n",
    "# 3. Ride length statistics\n",
    "print(f\"\\n3. Ride Length Statistics (minutes):\")\n",
    "for user_type in ['member', 'casual']:\n",
    "    user_data = clean_df[clean_df['member_casual'] == user_type]\n",
    "    print(f\"   {user_type.title()}:\")\n",
    "    print(f\"     Avg: {user_data['ride_length_min'].mean():.1f} min\")\n",
    "    print(f\"     Median: {user_data['ride_length_min'].median():.1f} min\")\n",
    "    print(f\"     Max: {user_data['ride_length_min'].max():.1f} min\")\n",
    "\n",
    "# 4. Missing values\n",
    "print(f\"\\n4. Missing Values in Clean Data:\")\n",
    "missing_clean = clean_df.isnull().sum()\n",
    "missing_clean = missing_clean[missing_clean > 0]\n",
    "if len(missing_clean) == 0:\n",
    "    print(\"   No missing values in critical columns\")\n",
    "else:\n",
    "    for col, count in missing_clean.items():\n",
    "        print(f\"   {col}: {count} missing ({count/len(clean_df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74cbaace-4a09-481c-80bd-29b86657e586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9. Saving cleaned data...\n",
      "Saved full cleaned data: 2025_cyclistic_cleaned_full.csv\n",
      "Saved analysis-ready data: 2025_cyclistic_analysis_ready.csv\n"
     ]
    }
   ],
   "source": [
    "# 9. Save cleaned data\n",
    "print(\"\\n9. Saving cleaned data...\")\n",
    "\n",
    "# Save full cleaned dataset\n",
    "clean_df.to_csv('2025_cyclistic_cleaned_full.csv', index=False)\n",
    "print(f\"Saved full cleaned data: 2025_cyclistic_cleaned_full.csv\")\n",
    "\n",
    "# Save analysis-ready summary (smaller, faster for analysis)\n",
    "analysis_cols = [\n",
    "    'ride_id', 'rideable_type', 'started_at', 'ended_at',\n",
    "    'member_casual', 'ride_length_min', 'day_name', \n",
    "    'month', 'year', 'hour', 'hour_category', 'season'\n",
    "]\n",
    "\n",
    "clean_df[analysis_cols].to_csv('2025_cyclistic_analysis_ready.csv', index=False)\n",
    "print(f\"Saved analysis-ready data: 2025_cyclistic_analysis_ready.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b36ae6cc-6fa9-4937-a8e6-5e9dc5086b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10. Creating summary for Excel visualization...\n",
      "Created Excel summary: 2025_cyclistic_summaries.xlsx\n"
     ]
    }
   ],
   "source": [
    "# 10. Create quick summary for Excel\n",
    "print(\"\\n10. Creating summary for Excel visualization...\")\n",
    "\n",
    "# Create summary tables for easy Excel import\n",
    "with pd.ExcelWriter('2025_cyclistic_summaries.xlsx') as writer:\n",
    "    \n",
    "    # 1. Monthly usage by user type\n",
    "    monthly_summary = pd.crosstab(\n",
    "        clean_df['month'], \n",
    "        clean_df['member_casual'],\n",
    "        values=clean_df['ride_id'],\n",
    "        aggfunc='count',\n",
    "        margins=True,\n",
    "        margins_name='Total'\n",
    "    )\n",
    "    monthly_summary.to_excel(writer, sheet_name='Monthly_Usage')\n",
    "    \n",
    "    # 2. Day of week patterns\n",
    "    dow_summary = pd.crosstab(\n",
    "        clean_df['day_name'], \n",
    "        clean_df['member_casual'],\n",
    "        values=clean_df['ride_length_min'],\n",
    "        aggfunc='mean'\n",
    "    ).round(2)\n",
    "    dow_summary.to_excel(writer, sheet_name='Day_of_Week')\n",
    "    \n",
    "    # 3. Hourly patterns\n",
    "    hourly_summary = pd.crosstab(\n",
    "        clean_df['hour'], \n",
    "        clean_df['member_casual'],\n",
    "        values=clean_df['ride_id'],\n",
    "        aggfunc='count'\n",
    "    )\n",
    "    hourly_summary.to_excel(writer, sheet_name='Hourly_Usage')\n",
    "    \n",
    "    # 4. Bike type preference\n",
    "    bike_summary = pd.crosstab(\n",
    "        clean_df['rideable_type'], \n",
    "        clean_df['member_casual'],\n",
    "        normalize='columns'\n",
    "    ).round(4) * 100\n",
    "    bike_summary.to_excel(writer, sheet_name='Bike_Preference')\n",
    "    \n",
    "    # 5. Basic statistics\n",
    "    stats_summary = clean_df.groupby('member_casual').agg({\n",
    "        'ride_id': 'count',\n",
    "        'ride_length_min': ['mean', 'median', 'std', 'min', 'max']\n",
    "    }).round(2)\n",
    "    stats_summary.to_excel(writer, sheet_name='Basic_Stats')\n",
    "\n",
    "print(f\"Created Excel summary: 2025_cyclistic_summaries.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8308b135-abf7-4e90-978d-5e06b2b08aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéâ CLEANING PROCESS COMPLETE!\n",
      "============================================================\n",
      "\n",
      "üìã FINAL RESULTS:\n",
      "   ‚Ä¢ Clean rides: 5,400,488\n",
      "   ‚Ä¢ Casual riders: 1,915,996\n",
      "   ‚Ä¢ Annual members: 3,484,492\n",
      "   ‚Ä¢ Data quality: 97.3% retained\n",
      "\n",
      "üìÅ OUTPUT FILES CREATED:\n",
      "   1. 2025_cyclistic_cleaned_full.csv       (Complete cleaned dataset)\n",
      "   2. 2025_cyclistic_analysis_ready.csv     (Analysis-optimized dataset)\n",
      "   3. 2025_cyclistic_summaries.xlsx         (Excel-ready summaries)\n",
      "\n",
      "üìä NEXT STEPS:\n",
      "   1. Open '2025_cyclistic_summaries.xlsx' for initial insights\n",
      "   2. Use cleaned data for in-depth analysis\n",
      "   3. Create visualizations comparing member vs. casual usage\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Final Report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ CLEANING PROCESS COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\"\"\n",
    "üìã FINAL RESULTS:\n",
    "   ‚Ä¢ Clean rides: {len(clean_df):,}\n",
    "   ‚Ä¢ Casual riders: {user_counts.get('casual', 0):,}\n",
    "   ‚Ä¢ Annual members: {user_counts.get('member', 0):,}\n",
    "   ‚Ä¢ Data quality: {(len(clean_df)/original_count*100):.1f}% retained\n",
    "\n",
    "üìÅ OUTPUT FILES CREATED:\n",
    "   1. 2025_cyclistic_cleaned_full.csv       (Complete cleaned dataset)\n",
    "   2. 2025_cyclistic_analysis_ready.csv     (Analysis-optimized dataset)\n",
    "   3. 2025_cyclistic_summaries.xlsx         (Excel-ready summaries)\n",
    "\n",
    "üìä NEXT STEPS:\n",
    "   1. Open '2025_cyclistic_summaries.xlsx' for initial insights\n",
    "   2. Use cleaned data for in-depth analysis\n",
    "   3. Create visualizations comparing member vs. casual usage\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb37c185-2a69-4c21-b23a-5f6ffe906256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
